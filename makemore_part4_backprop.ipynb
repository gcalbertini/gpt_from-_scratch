{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## makemore: becoming a backprop ninja\n",
    "\n",
    "swole doge style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there no change change in the first several cells from last lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = (\n",
    "    3  # context length: how many characters do we take to predict the next one?\n",
    ")\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])  # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])  # 10%\n",
    "Xte, Yte = build_dataset(words[n2:])  # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(\n",
    "        f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "    * (5 / 3)\n",
    "    / ((n_embd * block_size) ** 0.5)\n",
    ")\n",
    "b1 = (\n",
    "    torch.randn(n_hidden, generator=g) * 0.1\n",
    ")  # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size  # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 10]) torch.Size([32, 3]) torch.Size([32, 3, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  1,  4],\n",
       "         [18, 14,  1]]),\n",
       " tensor([[[-4.7125e-01,  7.8682e-01, -3.2843e-01, -4.3297e-01,  1.3729e+00,\n",
       "            2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4039e-01],\n",
       "          [-4.7125e-01,  7.8682e-01, -3.2843e-01, -4.3297e-01,  1.3729e+00,\n",
       "            2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4039e-01],\n",
       "          [-9.6478e-01, -2.3211e-01, -3.4762e-01,  3.3244e-01, -1.3263e+00,\n",
       "            1.1224e+00,  5.9641e-01,  4.5846e-01,  5.4011e-02, -1.7400e+00]],\n",
       " \n",
       "         [[ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
       "            1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00],\n",
       "          [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
       "            1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
       "          [-4.7125e-01,  7.8682e-01, -3.2843e-01, -4.3297e-01,  1.3729e+00,\n",
       "            2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4039e-01]]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(C.shape, Xb.shape, C[Xb].shape)\n",
    "Xb[0:2, :], C[Xb][0:2, :], b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3491, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb]  # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = (\n",
    "    1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    ")  # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)  # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2  # output layer\n",
    "# NOTE if we were doing binary classification this would be wrong as need p/(1-p) ratios for counts but softmax uses vectors of *all* classes so ok\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = (\n",
    "    counts_sum**-1\n",
    ")  # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [\n",
    "    logprobs,\n",
    "    probs,\n",
    "    counts,\n",
    "    counts_sum,\n",
    "    counts_sum_inv,  # afaik there is no cleaner way\n",
    "    norm_logits,\n",
    "    logit_maxes,\n",
    "    logits,\n",
    "    h,\n",
    "    hpreact,\n",
    "    bnraw,\n",
    "    bnvar_inv,\n",
    "    bnvar,\n",
    "    bndiff2,\n",
    "    bndiff,\n",
    "    hprebn,\n",
    "    bnmeani,\n",
    "    embcat,\n",
    "    emb,\n",
    "]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmeani.shape, hprebn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "dL_dlp = torch.zeros_like(logprobs)\n",
    "dL_dlp[range(n), Yb] = -1.0 / n\n",
    "assert dL_dlp.shape == logprobs.shape\n",
    "dlp_dp = 1.0 / probs\n",
    "dL_dp = dlp_dp * dL_dlp\n",
    "assert dL_dp.shape == probs.shape\n",
    "dp_dcsuminv = counts\n",
    "dL_csuminv = (dL_dp * dp_dcsuminv).sum(1, keepdim=True) # keepdim=True for [32,1] instead of just [32]\n",
    "assert dL_csuminv.shape == counts_sum_inv.shape\n",
    "\"\"\"cumulative sums add everything along row (spans column directions, left to right)\n",
    "\tHave something like where probs = counts_SUM_invertedOrNot[32x1] * dLdp[32x27] (csi broadcasts to [32x27]):\n",
    "\tcsi1*L11 csi1*L12 csi1*L13... ==> dprobs/dcsi[j] gives back counts entry corresponding to each j\n",
    "\tcsi2*L21 csi2*L22 csi2*L23... SUM csi2*L21 + csi2*L22 + csi2*L23 +... \t\n",
    "\tcsi3*L31 csi3*L32 csi3*L33... ==> csi3*L31 + csi3*L32 + csi3*L33 +... ==> dprobs/dsum version gives 1 else 0 in other rows to have 32x1\n",
    "\"\"\"\n",
    "dcsuminv_dcsum = -(counts_sum**-2)\n",
    "dcsum_dc = torch.ones_like(counts)\n",
    "dL_dcsum = dL_csuminv * dcsuminv_dcsum\n",
    "dL_dc = (\n",
    "    dL_dcsum * dcsum_dc + dL_dp * counts_sum_inv\n",
    ")  # latter part is other branch of counts before sum and inversion\n",
    "dc_dnormlogits = norm_logits.clone().exp()  # or just counts; derivative of e**x is e**x\n",
    "dL_dnormlogits = dc_dnormlogits * dL_dc\n",
    "assert dL_dnormlogits.shape == norm_logits.shape\n",
    "dnormlogits_dlogitmax = -torch.ones_like(\n",
    "    logit_maxes\n",
    ")  # I add ones_like for completeness for dx/dx, but can just be -dL_dnormlogits.sum(1, True) next...\n",
    "# logits max was a broadcasted scalar 32x1 --> 32x27, so reduce back\n",
    "dL_dlogitsmax = (dL_dnormlogits * dnormlogits_dlogitmax).sum(1, True)\n",
    "assert dL_dlogitsmax.shape == logit_maxes.shape\n",
    "dlogitsmax_dlogits = F.one_hot(\n",
    "    logits.max(1).indices, num_classes=logits.shape[1]\n",
    ").float()\n",
    "dL_dlogits = dL_dlogitsmax * dlogitsmax_dlogits + dL_dnormlogits * torch.ones_like(\n",
    "    logits\n",
    ")  # I add ones_like for completeness\n",
    "assert dL_dlogits.shape == logits.shape\n",
    "dlogits_dh = W2.T.clone()\n",
    "dL_dh = (\n",
    "    dL_dlogits @ dlogits_dh\n",
    ")  # 'regular' matrix multiplications with @, not element-wise Hadamard products (*)\n",
    "dlogits_dW2 = h.T.clone()\n",
    "dL_dW2 = dlogits_dW2 @ dL_dlogits\n",
    "# b2 bias is broadcasted to different rows for matrix ops; can then condense (keepdim False) to one item as 1 x n implies n\n",
    "dL_db2 = dL_dlogits.sum(0)\n",
    "dh_dhpreact = 1.0 - h**2\n",
    "dL_dhpreact = dL_dh * dh_dhpreact\n",
    "# bias term broadcasted row row addition so reduce back along rows\n",
    "dL_dbnbias = (dL_dhpreact * torch.ones_like(bnbias)).sum(0)\n",
    "# bias term broadcasted for element-wise mult along rows: bngain * bnraw ==> [1x64]*[32x64] -- keepdim T/F doesnt matter here as cmp-PyTorch comparison omits leading 1 in dim 0\n",
    "dL_dbngain = (dL_dhpreact * bnraw).sum(0)\n",
    "# this broadcasts for us as [32x64]*[1x64]\n",
    "dL_dbnraw = dL_dhpreact * bngain\n",
    "dbnraw_dbnvarinv = bndiff\n",
    "# bnvar_inv broadcasted along rows when bnraw = bnvar_inv[1x64]*bndiff[32x64], so reduce along dim 0 (broadcasted rows)\n",
    "dL_dbnvarinv = (dL_dbnraw * dbnraw_dbnvarinv).sum(0)\n",
    "dbnvarinv_droot = (-0.5 * (bnvar + 1e-5) ** -1.5) * torch.ones_like(bnvar)\n",
    "dL_dbnvar = dL_dbnvarinv * dbnvarinv_droot\n",
    "\"\"\"\n",
    "NOTE **sum along dim 0 in forward pass turns into replication (for broadcasting) along dim 0 in backward pass and vice versa**\n",
    "bndiff2-->32x64\n",
    "bnvar-->1x64\n",
    "bnvar = 1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    "\n",
    "bnvar1 = 1/(n-1)*(bndiff2_11+bndiff2_21+...)\n",
    "bnvar2 = 1/(n-1)*(bndiff2_12+bndiff2_22+...)\n",
    "\"\"\"\n",
    "dbnvar_dbndiff2 = (1.0 / (n - 1)) * torch.ones_like(bndiff2)  # [32x64]\n",
    "dL_dbndiff2 = dL_dbnvar * dbnvar_dbndiff2  # broadcasts as dL_dbnvar[1x64]\n",
    "dbndiff2_dndiff = 2 * bndiff\n",
    "dL_dbndiff = dL_dbnraw * bnvar_inv + dL_dbndiff2 * dbndiff2_dndiff\n",
    "# bndiff[32x64] = hprebn[32x64] - bnmeani[1x64]; bnmeani broadcasts along dim 0 so reduce\n",
    "dbndiff_dbnmeani = -torch.ones_like(bndiff)\n",
    "dL_dbnmeani = (dL_dbndiff * dbndiff_dbnmeani).sum(0)  # or -dL_dbndiff.sum(0)\n",
    "\"\"\"\n",
    "NOTE **sum along dim 0 in forward pass turns into replication (for broadcasting) along dim 0 in backward pass and vice versa**\n",
    "hprebn-->32x64\n",
    "bnmeani-->1x64\n",
    "bnmeani = 1 / n * hprebn[32x64].sum(0, keepdim=True)\n",
    "\n",
    "bnmeani1 = 1/(n-1)*(hprebn_11+hprebn_21+...)\n",
    "bnmeani2 = 1/(n-1)*(hprebn_12+hprebn_22+...)\n",
    "\"\"\"\n",
    "dbnmeani_dhprebn = (1.0 / n) * torch.ones_like(hprebn)  # broadcasts\n",
    "dbndiff_dhprebn = torch.ones_like(hprebn)\n",
    "dL_dhprebn = (\n",
    "    dL_dbnmeani * dbnmeani_dhprebn + dL_dbndiff * dbndiff_dhprebn\n",
    ")  # or dL_dbnmeani*dbnmeani_dhprebn+dL_dbndiff\n",
    "# hprebn[32x64] = embcat[32x30] @ W1[30, 64] + b1[64]\n",
    "dL_dembcat = dL_dhprebn @ W1.T\n",
    "dL_dW1 = embcat.T @ dL_dhprebn\n",
    "dhprebn_db1 = torch.ones_like(hprebn)  # broadcasted along rows so reduce element-wise\n",
    "dL_db1 = (dL_dhprebn * dhprebn_db1).sum(0)\n",
    "\n",
    "# emb[32x3x10] = C[Xb]\n",
    "# embcat[32,20] = emb.view(emb.shape[0], -1)\n",
    "dembcat_demb = torch.ones_like(embcat)\n",
    "dL_dem = (dL_dembcat * dembcat_demb).view(\n",
    "    emb.shape\n",
    ")  # going back to OG representation [32x3x10]\n",
    "\n",
    "# dem_dC undoes the indexing from C[27x10] and plugs back into Xb[32x3] to get back to [27x10] dimension\n",
    "dL_dC = torch.zeros_like(C)\n",
    "# along dim 0 (rows), accumulate the elements of dL_dem.view(-1, 10) using Xb as index points\n",
    "dL_dC.index_add_(\n",
    "    0, Xb.view(32 * 3), dL_dem.view(32 * 3, 10)\n",
    ")  # self[Xb.view(-1)[i], :] += dL_dem.view(-1, 10)[i, :]\n",
    "\n",
    "cmp(\"logprobs\", dL_dlp, logprobs)\n",
    "cmp(\"probs\", dL_dp, probs)\n",
    "cmp(\"counts_sum_inv\", dL_csuminv, counts_sum_inv)\n",
    "cmp(\"counts_sum\", dL_dcsum, counts_sum)\n",
    "cmp(\"counts\", dL_dc, counts)\n",
    "cmp(\"norm_logits\", dL_dnormlogits, norm_logits)\n",
    "cmp(\"logit_maxes\", dL_dlogitsmax, logit_maxes)\n",
    "cmp(\"logits\", dL_dlogits, logits)\n",
    "cmp(\"h\", dL_dh, h)\n",
    "cmp(\"W2\", dL_dW2, W2)\n",
    "cmp(\"b2\", dL_db2, b2)\n",
    "cmp(\"hpreact\", dL_dhpreact, hpreact)\n",
    "cmp(\"bnbias\", dL_dbnbias, bnbias)\n",
    "cmp(\"bngain\", dL_dbngain, bngain)\n",
    "cmp(\"bnraw\", dL_dbnraw, bnraw)\n",
    "cmp(\"bnvar_inv\", dL_dbnvarinv, bnvar_inv)\n",
    "cmp(\"bnvar\", dL_dbnvar, bnvar)\n",
    "cmp(\"bndiff2\", dL_dbndiff2, bndiff2)\n",
    "cmp(\"bndiff\", dL_dbndiff, bndiff)\n",
    "cmp(\"bnmeani\", dL_dbnmeani, bnmeani)\n",
    "cmp(\"hprebn\", dL_dhprebn, hprebn)\n",
    "cmp(\"embcat\", dL_dembcat, embcat)\n",
    "cmp(\"W1\", dL_dW1, W1)\n",
    "cmp(\"b1\", dL_db1, b1)\n",
    "cmp(\"emb\", dL_dem, emb)\n",
    "cmp(\"C\", dL_dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3491179943084717 diff: -2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), \"diff:\", (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.28642737865448e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp(\n",
    "    \"logits\", dlogits, logits\n",
    ")  # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0736, 0.0896, 0.0172, 0.0482, 0.0186, 0.0918, 0.0236, 0.0361, 0.0181,\n",
       "        0.0299, 0.0382, 0.0365, 0.0375, 0.0262, 0.0297, 0.0132, 0.0086, 0.0195,\n",
       "        0.0164, 0.0581, 0.0490, 0.0211, 0.0257, 0.0671, 0.0604, 0.0251, 0.0213],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0736,  0.0896,  0.0172,  0.0482,  0.0186,  0.0918,  0.0236,  0.0361,\n",
       "        -0.9819,  0.0299,  0.0382,  0.0365,  0.0375,  0.0262,  0.0297,  0.0132,\n",
       "         0.0086,  0.0195,  0.0164,  0.0581,  0.0490,  0.0211,  0.0257,  0.0671,\n",
       "         0.0604,  0.0251,  0.0213], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3283e-10, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dd0ff78c10>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATMAAAFgCAYAAADXQp4HAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkGUlEQVR4nO3df0xV9/kH8Dcg9wICl6HyawJD20qrwjKrlLR1tjKRZY1WmtgfybQxGh02U9a1YenvLaGzSevaUP2n0zSptTOpGk1m09KC6QZuMp0/S4XSqlOwWuFeQBDhfP/o17veCpz3xcPu9eP7ldxEL4+f87nnXB7Pvef5PCfCsiwLIiI3uMhQT0BExAlKZiJiBCUzETGCkpmIGEHJTESMoGQmIkZQMhMRIyiZiYgRxoR6At83MDCAM2fOICEhAREREaGejoiEkGVZ8Pl8yMjIQGTk8OdeYZfMzpw5g8zMzFBPQ0TCyKlTpzBx4sRhY0YtmVVVVeGVV15Ba2sr8vPz8cYbb2DWrFm2/y4hIQEAcODAAf+fhxIVFWU7Xnt7OzVft9tNxfX29trGJCUlUWN5vV7bGLv/ja6aPn06FXfo0CHHtsmuhGPOsPv7+x0b68qVK9RYLGZ/sPsiNjaWihsYGLCNuXz5MjUWs8/YebGvs6enx5GxOjs7cc8999jmAmCUktl7772H8vJybNy4EQUFBVi/fj2Ki4vR2NiIlJSUYf/t1R2fkJDgSDJjf0nYZOZyuWxjEhMTqbGYg8kmFvYjOfOmuFmSGTt/5n2mZBYoOjrasbEA7jWMygWAV199FcuXL8fjjz+OO+64Axs3bkRcXBz+/Oc/j8bmREScT2aXL19GQ0MDioqK/ruRyEgUFRWhrq7umvje3l54vd6Ah4hIsBxPZufPn0d/fz9SU1MDnk9NTUVra+s18ZWVlfB4PP6HvvwXkZEIeZ1ZRUUFOjo6/I9Tp06FekoicgNy/ALA+PHjERUVhba2toDn29rakJaWdk282+2mv3wXERmK42dmLpcLM2bMQHV1tf+5gYEBVFdXo7Cw0OnNiYgAGKXSjPLycixZsgR33nknZs2ahfXr16OrqwuPP/74aGxORGR0ktnixYvx9ddf47nnnkNrayt+/OMfY8+ePddcFBjOlStXbOuFmDobj8dDbY+t2Rkzxn6X+Xw+aiymlojZHgA0NTU5tk2mlg5wtjaMHSs3N9c25vPPP6fGYvYFwM2NrfNja+CcLPxl5s/uC6ZoHOBqFZltBrOkcdRWAKxevRqrV68ereFFRAKE/GqmiIgTlMxExAhKZiJiBCUzETGCkpmIGEHJTESMoGQmIkYIu7bZV/X09Ng2eGMK6piOl8FgigGZxnQA1xCPLRpkt8nsD3afMU0LAW6fscXBx44ds43JycmhxmpsbKTimH3LFp0mJydTcUzhdV9fHzUWgy3SZRt3OllozNKZmYgYQclMRIygZCYiRlAyExEjKJmJiBGUzETECEpmImIEJTMRMYKSmYgYIWxXAERGRtpWGzO3d2dbQLMV6Ay2BTeDvYW9k5XZ7GoCttU1g60GZ47nf/7zH2qsS5cuUXFMdT+7AqC9vZ2Kc/I9NGXKFNsYdjWEk8eJeW8H83upMzMRMYKSmYgYQclMRIygZCYiRlAyExEjKJmJiBGUzETECEpmImKEsC2azcvLs41pamqyjWGLTtm2wcx4bNEpU2jJzisuLo6KY4oQ2X3GFooyBb3s62T2bVpaGjXWF198QcWxhdcMttU4c5zYttlMQSx7zNn3NjM3JwvVAZ2ZiYghlMxExAhKZiJiBCUzETGCkpmIGEHJTESMoGQmIkZQMhMRIyiZiYgRwnYFwOHDh5GQkHDd4zhZcc1i2zEzLYhjY2OpsXp7e6k4ptW12+2mxmKr9pmVAmxlOXOcTp8+TY3FVr0zLazZ1RC33347FcesbmHf28wKDHY1AdvOOzEx0Tamp6eHGovl+JnZCy+8gIiIiIBHbm6u05sREQkwKmdmU6dOxUcfffTfjTi8BktE5PtGJcuMGTOGXuwrIuKEUbkAcOLECWRkZGDSpEl47LHHcPLkySFje3t74fV6Ax4iIsFyPJkVFBRg8+bN2LNnDzZs2ICWlhbce++98Pl8g8ZXVlbC4/H4H5mZmU5PSURuAhEWe0lnhNrb25GdnY1XX30Vy5Ytu+bnvb29AVfivF4vMjMzdTXz/7G9tNirTE5ezWSvoDJX09j9z8SxNydm588cp1BczXTyBtHs1UyWU1czfT4fpk6dio6ODtsxR/2b+aSkJNx2221DHhy3203/8oiIDGXUi2Y7OzvR3NyM9PT00d6UiNzEHE9mTz75JGpra/Hll1/i73//Ox588EFERUXhkUcecXpTIiJ+jn/MPH36NB555BFcuHABEyZMwD333IP6+npMmDAhqHGio6Ntq8K7u7ttx2G/c+rq6qLimO/g2O8yYmJiHBuL/c5pypQptjHHjh1zdJvM90nsd35MXHx8PDUWs/8B7jtQ9vs35rswgDvu7P5nxmK+Fwxmm52dnY5sk/3+ExiFZLZ161anhxQRsaWF5iJiBCUzETGCkpmIGEHJTESMoGQmIkZQMhMRIyiZiYgRwrZr4pUrV2zbMjMFfOyi75SUFCru/PnztjFsoS6z0JZZsAvwRb9HjhyxjWEWJgP84mRmPHZ9LtNVxcnCVBZbdMoeTydbYTHtzdmGDGwRK3M8mfcPu18BnZmJiCGUzETECEpmImIEJTMRMYKSmYgYQclMRIygZCYiRlAyExEjKJmJiBHCdgVAZGSkbeU4046ZrfL+5ptvqDimAjo3N5caq7m5mYpjsLc6Y6rxnW6hzGyTbZt94sQJ2xh2/k6+TqbKPhjM3NhVE8x7w+l9xqy8cfL2joDOzETEEEpmImIEJTMRMYKSmYgYQclMRIygZCYiRlAyExEjKJmJiBGUzETECGG7AoC5B0BOTo7tOF988QW1Pba3OVO1zFSps9v0+XzUWB6Ph4pjKrO7u7upsaKjo6k4ZhUGWw3OVNqz9zBgK+iZ48Tui/b2diouNjbWNqazs5Mai3mdzP0oAP5eAU6tmghmZYXOzETECEpmImIEJTMRMYKSmYgYQclMRIygZCYiRlAyExEjKJmJiBHCtmi2v7/ftljx888/tx2HLaBk45gCSrZVN1MQyLbDZgsoGWxhJFvQyBSAsm2zmbllZGRQY7W1tVFxzHuDLZplipYBIDs72zbm6NGj1FhMETT7/mfbZjO/J8w22XkBIzgz27t3Lx544AFkZGQgIiICO3bsCPi5ZVl47rnnkJ6ejtjYWBQVFdEV8SIiIxV0Muvq6kJ+fj6qqqoG/fm6devw+uuvY+PGjdi3bx/Gjh2L4uJiermEiMhIBP0xs6SkBCUlJYP+zLIsrF+/Hs888wwWLFgAAHj77beRmpqKHTt24OGHH76+2YqIDMHRCwAtLS1obW1FUVGR/zmPx4OCggLU1dUN+m96e3vh9XoDHiIiwXI0mbW2tgIAUlNTA55PTU31/+z7Kisr4fF4/I/MzEwnpyQiN4mQl2ZUVFSgo6PD/zh16lSopyQiNyBHk1laWhqAay95t7W1+X/2fW63G4mJiQEPEZFgOZrMcnJykJaWhurqav9zXq8X+/btQ2FhoZObEhEJEPTVzM7OTjQ1Nfn/3tLSgoMHDyI5ORlZWVlYs2YN/vCHP+DWW29FTk4Onn32WWRkZGDhwoVOzltEJEDQyWz//v247777/H8vLy8HACxZsgSbN2/GU089ha6uLqxYsQLt7e245557sGfPHsTExAS1nYiICNtqY5fLZTsOW6X+i1/8gorbtWuXbUx8fDw1lpPzZzGV2eyqA7YanKkxZMfq6+uzjfnyyy+psdiVDkwcW9nPrIYAgObmZtsYttU78x5i25azx4kZj1n1wb4XgREkszlz5gy7XCciIgIvvfQSXnrppWCHFhEZsZBfzRQRcYKSmYgYQclMRIygZCYiRlAyExEjKJmJiBGUzETECGHbNtuyLNv200wBJVusu3v3biqOaePb1dVFjcWsQ2VeIwDccccdVNxnn31mG8MWY7KFlgwnC3Xdbjc1FhvX29trG8O2zWbGYsdjW0onJSXZxly8eJEay8n22kwxMlvYDOjMTEQMoWQmIkZQMhMRIyiZiYgRlMxExAhKZiJiBCUzETGCkpmIGEHJTESMELYrAJi22Ww1Mrs9BlOpzrbN7uzsdGR7AHD06FEqzm5VBcBXXTNjAVx7cKaFMgBMmzbNNua796gYTnd3NxXHvDfGjh1LjdXR0UHFMceAnT+zb9kVDCz298lOML/jOjMTESMomYmIEZTMRMQISmYiYgQlMxExgpKZiBhByUxEjKBkJiJGUDITESOE7QoAl8tlWznO9Mdne+gzVeoA18Od7fPOVDez9zBg+/azVfsMtjp70qRJtjHMvQkA4Pjx47Yx7DF3cgUDs5oDAGJjY6k45niy9zBg9weDfZ8xmFUCwWxPZ2YiYgQlMxExgpKZiBhByUxEjKBkJiJGUDITESMomYmIEZTMRMQIYVs0m5+fb1tU9+WXX9qOwxawsnEMtoWyz+ezjenp6aHGYtsUjxljf8jZYlh2m83NzbYxly5dosZi5sa2GmdbRTNtp9liWPZ1Mm2z2dfJHCe2AJfdJrPPmLGCKfIO+sxs7969eOCBB5CRkYGIiAjs2LEj4OdLly719++/+pg/f36wmxERCUrQyayrqwv5+fmoqqoaMmb+/Pk4e/as//Huu+9e1yRFROwE/TGzpKQEJSUlw8a43W6kpaWNeFIiIsEalQsANTU1SElJwZQpU7Bq1SpcuHBhyNje3l54vd6Ah4hIsBxPZvPnz8fbb7+N6upq/PGPf0RtbS1KSkqGXP1eWVkJj8fjf2RmZjo9JRG5CTh+NfPhhx/2/3n69OnIy8vD5MmTUVNTg7lz514TX1FRgfLycv/fvV6vEpqIBG3U68wmTZqE8ePHD3mXabfbjcTExICHiEiwRj2ZnT59GhcuXEB6evpob0pEbmJBf8zs7OwMOMtqaWnBwYMHkZycjOTkZLz44osoLS1FWloampub8dRTT+GWW25BcXGxoxMXEfmuCCvIPso1NTW47777rnl+yZIl2LBhAxYuXIgDBw6gvb0dGRkZmDdvHn7/+98jNTWVGt/r9cLj8eDQoUNISEgYNpaZOluNz1QsA1xlNtvql1l1wFbjs4eRiWOrwbOysqi4lpYW2xhmvwLcCgZWV1eXY2Oxx4lddXDlyhXbGPZ9xlTas+3ZmXkB3HFijrnP50Nubi46Ojpsv4IK+p0xZ86cYX8hPvjgg2CHFBG5blpoLiJGUDITESMomYmIEZTMRMQISmYiYgQlMxExgpKZiBhByUxEjBC29wCYOXOmbe/yr776ynYctrKf1dfXZxvDVuMzvdnZFQzd3d1UHFM1zlapf/7551QcU4HOVpYzY7F96tnjxFSzs9X47EoB5jWwKzWY9yz7e8LOn8G8/9n7TAA6MxMRQyiZiYgRlMxExAhKZiJiBCUzETGCkpmIGEHJTESMoGQmIkYI26LZ+vp627bZzA2D2XbATAtrgCsaZIs2mTtRXbp0iRrLybbHnZ2d1Fgul4uKY7DFkUxxJ1tMGhcXR8U52d6cfZ8x+5Yt+mXeZxcvXqTGYrfJvM9ycnIc2x6gMzMRMYSSmYgYQclMRIygZCYiRlAyExEjKJmJiBGUzETECEpmImIEJTMRMULYrgCIjIy0rapmqoPZdswsptKbrQZn5saOxbY9njx5sm3MiRMnqLHYuTm5aoLZZ0ybaIBvdc3Mjd0XSUlJVFxXV5dtDLvPmLGcXEECcHNj3mc+nw/5+fnUNnVmJiJGUDITESMomYmIEZTMRMQISmYiYgQlMxExgpKZiBhByUxEjBC2RbNut9u2/XF3d7ftOGzb3ejoaCqOKQaMioqixurp6bGNYYsx2fk3NjbaxoSi1Thb9Mu0k2bn7/P5qDimpTd7nNg26Mz+YLfJFAezbcvZ9/bUqVNtY44fP+7Y9gCdmYmIIYJKZpWVlZg5cyYSEhKQkpKChQsXXvM/fU9PD8rKyjBu3DjEx8ejtLQUbW1tjk5aROT7gkpmtbW1KCsrQ319PT788EP09fVh3rx5AWu/1q5di127dmHbtm2ora3FmTNnsGjRIscnLiLyXUF9Z7Znz56Av2/evBkpKSloaGjA7Nmz0dHRgbfeegtbtmzB/fffDwDYtGkTbr/9dtTX1+Ouu+5ybuYiIt9xXd+ZdXR0AACSk5MBAA0NDejr60NRUZE/Jjc3F1lZWairqxt0jN7eXni93oCHiEiwRpzMBgYGsGbNGtx9992YNm0aAKC1tRUul+uaNiepqalobW0ddJzKykp4PB7/IzMzc6RTEpGb2IiTWVlZGY4cOYKtW7de1wQqKirQ0dHhf5w6deq6xhORm9OI6sxWr16N3bt3Y+/evZg4caL/+bS0NFy+fBnt7e0BZ2dtbW1IS0sbdCymnkxExE5QZ2aWZWH16tXYvn07Pv74Y+Tk5AT8fMaMGYiOjkZ1dbX/ucbGRpw8eRKFhYXOzFhEZBBBnZmVlZVhy5Yt2LlzJxISEvzfg3k8HsTGxsLj8WDZsmUoLy9HcnIyEhMT8cQTT6CwsDDoK5n5+fm2VcktLS224zjdNpupph4zhtutzFhslTdbje9kq3G2bTMzN/Z1MtjVBGzVO3M82X2WkJBAxbErBRjMcWLfs+yKmqNHjzoyFrs9IMhktmHDBgDAnDlzAp7ftGkTli5dCgB47bXXEBkZidLSUvT29qK4uBhvvvlmMJsREQlaUMmMyZIxMTGoqqpCVVXViCclIhIsrc0UESMomYmIEZTMRMQISmYiYgQlMxExgpKZiBhByUxEjBC29wCor6+3rZZOT0+3HYdduO5kP3vm3gQAkJiYaBvDVoLHxsZScUylOrsv2PsOMNjVBEx1P7vWNz4+nopj9gdbQd/e3k7FMa+BrY5nVh1cvHiRGotdqcGurrATzAoAnZmJiBGUzETECEpmImIEJTMRMYKSmYgYQclMRIygZCYiRlAyExEjhG3RLHOjE6Ywr6+vj9oeW5zHFDOyRadMAStbTNrT00PFMUWPTrdQZrBFlkyhLjsWG8ccp6ioKGosJ4uDWczc2H0RExNDxTHzZ9rGMzFX6cxMRIygZCYiRlAyExEjKJmJiBGUzETECEpmImIEJTMRMYKSmYgYQclMRIwQtisA+vv7bat/29rabMfx+XzU9tjKZqa6n21hzbTXnjJlCjXWiRMnqDimojo5OZka6/z581QcU4HOVNkD3AoAtnqeXanBYCvV2dUVzP5gW1gzxykzM5Ma6+uvv6bimNUhzGqaYFZC6MxMRIygZCYiRlAyExEjKJmJiBGUzETECEpmImIEJTMRMYKSmYgYQclMRIwQtisAmHsAdHZ22o7jdM91puqaqVIHuMr4pqYmaiy2Hz8z/4sXL1JjMRXcLCf78bP7gj1OTHX/9OnTqbEOHz5MxTHvDfZ1xsfH28awlf3sPmPmxty3gr23BRDkmVllZSVmzpyJhIQEpKSkYOHChWhsbAyImTNnDiIiIgIeK1euDGYzIiJBCyqZ1dbWoqysDPX19fjwww/R19eHefPmoaurKyBu+fLlOHv2rP+xbt06RyctIvJ9QX3M3LNnT8DfN2/ejJSUFDQ0NGD27Nn+5+Pi4pCWlubMDEVECNd1AaCjowPAtV0W3nnnHYwfPx7Tpk1DRUXFsN0hent74fV6Ax4iIsEa8QWAgYEBrFmzBnfffTemTZvmf/7RRx9FdnY2MjIycOjQITz99NNobGzE+++/P+g4lZWVePHFF0c6DRERANeRzMrKynDkyBF8+umnAc+vWLHC/+fp06cjPT0dc+fORXNzMyZPnnzNOBUVFSgvL/f/3ev10r2VRESuGlEyW716NXbv3o29e/di4sSJw8YWFBQA+LbEYLBkxpRgiIjYCSqZWZaFJ554Atu3b0dNTQ1ycnJs/83BgwcBAOnp6SOaoIgII6hkVlZWhi1btmDnzp1ISEhAa2srAMDj8SA2NhbNzc3YsmULfv7zn2PcuHE4dOgQ1q5di9mzZyMvLy+oifX19aGvry+ofzMYtrUwW1zLFA1evTBiJyEhwTaGLRpk58+04T569Cg1FlPYycaxbbOZ48nuC5fLRcUx7bWPHTtGjcXuM2Z/sC24maLZc+fOUWOx+5ZtI+6koJLZhg0bAHxbGPtdmzZtwtKlS+FyufDRRx9h/fr16OrqQmZmJkpLS/HMM884NmERkcEE/TFzOJmZmaitrb2uCYmIjIQWmouIEZTMRMQISmYiYgQlMxExgpKZiBhByUxEjKBkJiJGCNu22VeuXKGrwofDtvnNzs6m4lpaWmxj2BbQly5dso1hK67ZynKmDTfbQpw9PsxrYPcZ8zrZY87On620ZzCrCYBr22oN5ptvvqHGYtqgs+8zdlUOs8+Y4xTMKiCdmYmIEZTMRMQISmYiYgQlMxExgpKZiBhByUxEjKBkJiJGUDITESOEbdFsbGwsYmNjh41hijvZAlCmmBSwb1AJfHtXKsbx48dtY9i232wxppNFp2xrZKYgk9mv7FhsAWhcXBwV19nZaRsTExNDjcUWNzP3j2XHYrD7gn1vMK3jmfcP+/sL6MxMRAyhZCYiRlAyExEjKJmJiBGUzETECEpmImIEJTMRMYKSmYgYQclMRIwQtisAuru7bSucmapxtkraybbNhw8fpsZiWguzlf1JSUlUXFpamm0MuxqC3WcMdqUDc8zZavyuri4qjhFMe2cGsz/Ytt/Mceru7qbGYlcAMMeAWakRTMtynZmJiBGUzETECEpmImIEJTMRMYKSmYgYQclMRIygZCYiRlAyExEjKJmJiBHCdgXAnXfeaVu5/MUXX9iOw1Zms1XjzHhslTTT35ytsmf61ANAY2OjY9tk7wHAVO2zKwCYbfb09FBjObmCgR3L5XJRcex9DBjM+ywhIYEaiz1OPp/PNobZZ+x7DAjyzGzDhg3Iy8tDYmIiEhMTUVhYiL/+9a/+n/f09KCsrAzjxo1DfHw8SktL0dbWFswmRERGJKhkNnHiRLz88stoaGjA/v37cf/992PBggU4evQoAGDt2rXYtWsXtm3bhtraWpw5cwaLFi0alYmLiHxXhMXe42sIycnJeOWVV/DQQw9hwoQJ2LJlCx566CEAwGeffYbbb78ddXV1uOuuu6jxvF4vPB4PxowZ8z/9mGl3W7tgxnP6YxqDPYzMNtnFveH6MZP9mM9ysqEBG8d8zGTf28z8x44dS431v/6Y6fP5kJ+fj46ODiQmJg4/N2pmg+jv78fWrVvR1dWFwsJCNDQ0oK+vD0VFRf6Y3NxcZGVloa6ubshxent74fV6Ax4iIsEKOpkdPnwY8fHxcLvdWLlyJbZv34477rgDra2tcLlc17SiSU1NRWtr65DjVVZWwuPx+B+ZmZlBvwgRkaCT2ZQpU3Dw4EHs27cPq1atwpIlS3Ds2LERT6CiogIdHR3+x6lTp0Y8lojcvIIuzXC5XLjlllsAADNmzMA///lP/OlPf8LixYtx+fJltLe3B5ydtbW1DdsQ0O12w+12Bz9zEZHvuO6i2YGBAfT29mLGjBmIjo5GdXW1/2eNjY04efIkCgsLr3czIiLDCurMrKKiAiUlJcjKyoLP58OWLVtQU1ODDz74AB6PB8uWLUN5eTmSk5ORmJiIJ554AoWFhfSVzO/697//bVvIx1zNYc/62LbB8fHxjo3FXJljrx6xRZbM/mDbMbNzY8TFxVFxTEEsezXZyaLZq59W7FwtY7LDXF1nryYz+9bJFuIAd0WceZ8FU2wRVDI7d+4cfvnLX+Ls2bPweDzIy8vDBx98gJ/97GcAgNdeew2RkZEoLS1Fb28viouL8eabbwazCRGREQkqmb311lvD/jwmJgZVVVWoqqq6rkmJiARLC81FxAhKZiJiBCUzETGCkpmIGEHJTESMoGQmIkYIu06zV4vkmM6pTNEd02UT4LuTMkV84Vw0y+wPtmjWSWwBaLgWzbLFnUxrHIA7Bk6+z9j3P8upotmreYDZv9fdz8xpp0+fVucMEQlw6tQpTJw4cdiYsEtmAwMDOHPmDBISEvz/c3q9XmRmZuLUqVO2DdrCkeYfejf6a7hZ529ZFnw+HzIyMmw/pYTdx8zIyMghM/DVew/cqDT/0LvRX8PNOH+Px0PF6QKAiBhByUxEjHBDJDO3243nn3/+hm3iqPmH3o3+GjR/e2F3AUBEZCRuiDMzERE7SmYiYgQlMxExgpKZiBjhhkhmVVVV+NGPfoSYmBgUFBTgH//4R6inRHnhhRcQERER8MjNzQ31tIa0d+9ePPDAA8jIyEBERAR27NgR8HPLsvDcc88hPT0dsbGxKCoqwokTJ0Iz2UHYzX/p0qXXHI/58+eHZrKDqKysxMyZM5GQkICUlBQsXLgQjY2NATE9PT0oKyvDuHHjEB8fj9LSUrS1tYVoxoGY+c+ZM+eaY7By5UpHth/2yey9995DeXk5nn/+efzrX/9Cfn4+iouLce7cuVBPjTJ16lScPXvW//j0009DPaUhdXV1IT8/f8h7OKxbtw6vv/46Nm7ciH379mHs2LEoLi52fJHySNnNHwDmz58fcDzefffd/+EMh1dbW4uysjLU19fjww8/RF9fH+bNmxdw56S1a9di165d2LZtG2pra3HmzBksWrQohLP+L2b+ALB8+fKAY7Bu3TpnJmCFuVmzZlllZWX+v/f391sZGRlWZWVlCGfFef755638/PxQT2NEAFjbt2/3/31gYMBKS0uzXnnlFf9z7e3tltvttt59990QzHB435+/ZVnWkiVLrAULFoRkPiNx7tw5C4BVW1trWda3+zs6Otratm2bP+b48eMWAKuuri5U0xzS9+dvWZb105/+1Pr1r389KtsL6zOzy5cvo6GhAUVFRf7nIiMjUVRUhLq6uhDOjHfixAlkZGRg0qRJeOyxx3Dy5MlQT2lEWlpa0NraGnAsPB4PCgoKbphjAQA1NTVISUnBlClTsGrVKly4cCHUUxpSR0cHACA5ORkA0NDQgL6+voBjkJubi6ysrLA8Bt+f/1XvvPMOxo8fj2nTpqGiooJuZWQn7Baaf9f58+fR39+P1NTUgOdTU1Px2WefhWhWvIKCAmzevBlTpkzB2bNn8eKLL+Lee+/FkSNHbG9wHG5aW1sBYNBjcfVn4W7+/PlYtGgRcnJy0NzcjN/97ncoKSlBXV0doqKiQj29AAMDA1izZg3uvvtuTJs2DcC3x8DlciEpKSkgNhyPwWDzB4BHH30U2dnZyMjIwKFDh/D000+jsbER77///nVvM6yT2Y2upKTE/+e8vDwUFBQgOzsbf/nLX7Bs2bIQzuzm9PDDD/v/PH36dOTl5WHy5MmoqanB3LlzQziza5WVleHIkSNh/R3rcIaa/4oVK/x/nj59OtLT0zF37lw0Nzdj8uTJ17XNsP6YOX78eERFRV1ztaatrQ1paWkhmtXIJSUl4bbbbkNTU1OopxK0q/vblGMBAJMmTcL48ePD7nisXr0au3fvxieffBLQDistLQ2XL19Ge3t7QHy4HYOh5j+YgoICAHDkGIR1MnO5XJgxYwaqq6v9zw0MDKC6uhqFhYUhnNnIdHZ2orm5Genp6aGeStBycnKQlpYWcCy8Xi/27dt3Qx4L4NuuxhcuXAib42FZFlavXo3t27fj448/Rk5OTsDPZ8yYgejo6IBj0NjYiJMnT4bFMbCb/2AOHjwIAM4cg1G5rOCgrVu3Wm6329q8ebN17Ngxa8WKFVZSUpLV2toa6qnZ+s1vfmPV1NRYLS0t1t/+9jerqKjIGj9+vHXu3LlQT21QPp/POnDggHXgwAELgPXqq69aBw4csL766ivLsizr5ZdftpKSkqydO3dahw4dshYsWGDl5ORYly5dCvHMvzXc/H0+n/Xkk09adXV1VktLi/XRRx9ZP/nJT6xbb73V6unpCfXULcuyrFWrVlkej8eqqamxzp496390d3f7Y1auXGllZWVZH3/8sbV//36rsLDQKiwsDOGs/8tu/k1NTdZLL71k7d+/32ppabF27txpTZo0yZo9e7Yj2w/7ZGZZlvXGG29YWVlZlsvlsmbNmmXV19eHekqUxYsXW+np6ZbL5bJ++MMfWosXL7aamppCPa0hffLJJxaAax5LliyxLOvb8oxnn33WSk1NtdxutzV37lyrsbExtJP+juHm393dbc2bN8+aMGGCFR0dbWVnZ1vLly8Pq/8UB5s7AGvTpk3+mEuXLlm/+tWvrB/84AdWXFyc9eCDD1pnz54N3aS/w27+J0+etGbPnm0lJydbbrfbuuWWW6zf/va3VkdHhyPbVwsgETFCWH9nJiLCUjITESMomYmIEZTMRMQISmYiYgQlMxExgpKZiBhByUxEjKBkJiJGUDITESMomYmIEZTMRMQI/we33mDbFuJOnQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = (\n",
    "    bngain\n",
    "    * (hprebn - hprebn.mean(0, keepdim=True))\n",
    "    / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5)\n",
    "    + bnbias\n",
    ")\n",
    "print(\"max diff:\", (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "dhpreact = dL_dhpreact\n",
    "dhprebn = (\n",
    "    bngain\n",
    "    * bnvar_inv\n",
    "    / n\n",
    "    * (n * dhpreact - dhpreact.sum(0) - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    ")\n",
    "\n",
    "cmp(\n",
    "    \"hprebn\", dhprebn, hprebn\n",
    ")  # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000/ 200000: 2.1824\n",
      "  20000/ 200000: 2.3904\n",
      "  30000/ 200000: 2.4525\n",
      "  40000/ 200000: 1.9773\n",
      "  50000/ 200000: 2.3544\n",
      "  60000/ 200000: 2.4329\n",
      "  70000/ 200000: 2.0275\n",
      "  80000/ 200000: 2.3596\n",
      "  90000/ 200000: 2.0401\n",
      " 100000/ 200000: 1.9544\n",
      " 110000/ 200000: 2.3391\n",
      " 120000/ 200000: 2.0384\n",
      " 130000/ 200000: 2.4563\n",
      " 140000/ 200000: 2.2741\n",
      " 150000/ 200000: 2.1285\n",
      " 160000/ 200000: 1.9298\n",
      " 170000/ 200000: 1.8868\n",
      " 180000/ 200000: 2.1026\n",
      " 190000/ 200000: 1.9751\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "    * (5 / 3)\n",
    "    / ((n_embd * block_size) ** 0.5)\n",
    ")\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size  # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "    # kick off optimization\n",
    "    for i in range(max_steps):\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[Xb]  # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "        # Linear layer\n",
    "        hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "        # BatchNorm layer\n",
    "        # -------------------------------------------------------------\n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "        bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # -------------------------------------------------------------\n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact)  # hidden layer\n",
    "        logits = h @ W2 + b2  # output layer\n",
    "        loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "        # manual backprop! #swole_doge_meme\n",
    "        # -----------------\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n), Yb] -= 1\n",
    "        dlogits /= n\n",
    "        # 2nd layer backprop\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "        # tanh\n",
    "        dhpreact = (1.0 - h**2) * dh\n",
    "        # batchnorm backprop\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = (\n",
    "            bngain\n",
    "            * bnvar_inv\n",
    "            / n\n",
    "            * (\n",
    "                n * dhpreact\n",
    "                - dhpreact.sum(0)\n",
    "                - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0)\n",
    "            )\n",
    "        )\n",
    "        # 1st layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # embedding\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k, j]\n",
    "                dC[ix] += demb[k, j]\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        # -----------------\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01  # step learning rate decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "            p.data += -lr * grad  # new way of swole doge TODO: enable\n",
    "\n",
    "        # track stats\n",
    "        if i % 10000 == 0:  # print every once in a while\n",
    "            print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "    #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0716607570648193\n",
      "val 2.1087400913238525\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "\n",
    "@torch.no_grad()  # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"val\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]  # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5) ** -0.5 + bnbias\n",
    "    h = torch.tanh(hpreact)  # (N, n_hidden)\n",
    "    logits = h @ W2 + b2  # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "mayah.\n",
      "see.\n",
      "mad.\n",
      "rylle.\n",
      "emmani.\n",
      "jarlee.\n",
      "adelynnelin.\n",
      "shi.\n",
      "jen.\n",
      "eden.\n",
      "estanaraelynn.\n",
      "hokalin.\n",
      "shubergihirael.\n",
      "kindreelynn.\n",
      "novana.\n",
      "uba.\n",
      "ged.\n",
      "ryyah.\n",
      "faeh.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # ------------\n",
    "        # forward pass:\n",
    "        # Embedding\n",
    "        emb = C[torch.tensor([context])]  # (1,block_size,d)\n",
    "        embcat = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5) ** -0.5 + bnbias\n",
    "        h = torch.tanh(hpreact)  # (N, n_hidden)\n",
    "        logits = h @ W2 + b2  # (N, vocab_size)\n",
    "        # ------------\n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
